{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert pdfs tp text files\n",
    "\n",
    "* This is to convert the documents from the Jornal of Extension to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textractor import Textractor\n",
    "from textractor.visualizers.entitylist import EntityList\n",
    "from textractor.data.constants import TextractFeatures\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "from glob import glob\n",
    "sys.path.append(\"..\")\n",
    "import utils as ut\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        return chardet.detect(file.read())['encoding']\n",
    "    \n",
    "\n",
    "def html_to_text_file_joe(html_file, out_dir):\n",
    "    \"\"\"Convert journal of extension-related html file to text file\"\"\"\n",
    "\n",
    "    fname = html_file.split(os.path.sep)[-1].replace(\"html\", \"txt\")\n",
    "    output_file = os.path.join(out_dir, f\"{fname}\")\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"\\html_to_text_file_joe(): {output_file} exists already. Skipping\\n\")\n",
    "        return\n",
    "\n",
    "    encoding = get_encoding(html_file)\n",
    "\n",
    "    # Read the HTML file\n",
    "    with open(html_file, 'r', encoding=encoding) as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Define tags to be skipped\n",
    "    skip_tags = ['DIV-author', 'DIV-copyright-footer', 'footer', 'footer-container']\n",
    "\n",
    "    # Split paragraphs into sentences and write each sentence in a new line\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for p in soup.find_all('p'):\n",
    "            if not any(parent.has_attr('class') and skip_tag in parent['class'] for parent in p.parents for skip_tag in skip_tags):\n",
    "                if not any(parent.has_attr('id') and parent['id'] in skip_tags for parent in p.parents):\n",
    "                    # Check if paragraph contains any links (<a> tags), if so, skip it\n",
    "                    if p.find('a'):\n",
    "                        continue\n",
    "\n",
    "                    # Remove newlines within a paragraph and strip leading/trailing spaces\n",
    "                    paragraph_text = ' '.join(p.get_text().split())\n",
    "                    # Splitting the paragraph into sentences\n",
    "                    sentences = re.split(r'(?<=[.!?])\\s+', paragraph_text)\n",
    "                    for sentence in sentences:\n",
    "                        file.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text_file_joe(pdf_file_path, extractor_obj, s3_bucket_name, out_dir):\n",
    "    got_text = ut.pdf2text_aws_textract(pdf_file_path, extractor_obj, s3_bucket_name, out_dir)\n",
    "\n",
    "    fname = pdf_file_path.split(os.path.sep)[-1].replace(\"pdf\", \"txt\")\n",
    "    output_file = os.path.join(out_dir, f\"{fname}\")\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"\\npdf_to_text_file_joe(): {output_file} exists already. Skipping\\n\")\n",
    "        return\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as text_file:\n",
    "            # Remove newlines within a paragraph and strip leading/trailing spaces\n",
    "            paragraph_text = ' '.join(got_text.split())\n",
    "            # Splitting the paragraph into sentences\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', paragraph_text)\n",
    "            for sentence in sentences:\n",
    "                text_file.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_region = 'us-east-1'\n",
    "s3_client = boto3.client('s3', region_name=aws_region)\n",
    "textract_client = boto3.client('textract', region_name=aws_region)\n",
    "\n",
    "extractor_obj = Textractor(region_name=aws_region)\n",
    "s3_bucket_name = \"ae-corpora-bucket\"\n",
    "out_dir = r\"./for_mlm_ae_corpus/joe_converted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joe_raw_fpath = r\"./for_mlm_ae_corpus/joe_raw\"\n",
    "joe_converted_outdir = r\"./for_mlm_ae_corpus/joe_converted\"\n",
    "\n",
    "joe_raw_files_list = glob(f\"{joe_raw_fpath}/*\")\n",
    "\n",
    "start_time = time.monotonic()\n",
    "for i, file in enumerate(joe_raw_files_list, start=1):\n",
    "    fext = file.split(\".\")[-1]\n",
    "    if fext in [\"pdf\", \"PDF\"]:\n",
    "        pdf_to_text_file_joe(file, extractor_obj, s3_bucket_name, out_dir)\n",
    "    elif fext in [\"html\", \"htm\"]:\n",
    "        html_to_text_file_joe(file, out_dir)\n",
    "    else:\n",
    "        print(f\"File extension not recognized: {file[-3:]}. Skipping\")\n",
    "\n",
    "duration = timedelta(seconds=time.monotonic() - start_time)\n",
    "print(f\"Conversion took: {duration}\")    \n",
    "\n",
    "joe_conv_files_list = glob(f\"{joe_converted_outdir}/*\")\n",
    "\n",
    "print(f\"To convert: {len(joe_raw_files_list)}\")\n",
    "print(f\"Converted : {len(joe_conv_files_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "001_aequad_benchmark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

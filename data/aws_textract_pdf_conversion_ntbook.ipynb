{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert pdfs tp text files\n",
    "\n",
    "* Requirement: a AWS Account and an Amazon Textract license\n",
    "* This is to convert the land-grant universities- related documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USeful links:\n",
    "* https://github.com/aws-samples/amazon-textract-textractor\n",
    "* https://aws-samples.github.io/amazon-textract-textractor/notebooks/simple_ocr.html\n",
    "* https://aws-samples.github.io/amazon-textract-textractor/notebooks/layout_analysis_for_text_linearization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from glob import glob\n",
    "import os\n",
    "import utils as ut\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_main_dir = \"./for_mlm_ae_corpus/ext_univs_raw/pdfs\"\n",
    "CLEAN_OUT_DIR = \"./for_mlm_ae_corpus/ext_univs_converted\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. get some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0, Massachusetts     , npdfs =  0        , total_pdfpages = 0\n",
      " 1, New_Hampshire     , npdfs =  0        , total_pdfpages = 0\n",
      " 2, Pennsylvania      , npdfs =  0        , total_pdfpages = 0\n",
      " 3, Maine             , npdfs =  0        , total_pdfpages = 0\n",
      " 4, Nebraska          , npdfs =  0        , total_pdfpages = 0\n",
      " 5, Maryland          , npdfs =  3        , total_pdfpages = 16\n",
      " 6, North_Dakota      , npdfs =  5        , total_pdfpages = 52\n",
      " 7, Wisconsin         , npdfs =  34       , total_pdfpages = 578\n",
      " 8, South_Carolina    , npdfs =  0        , total_pdfpages = 0\n",
      " 9, Tennessee         , npdfs =  31       , total_pdfpages = 378\n",
      "10, New_Mexico        , npdfs =  1        , total_pdfpages = 22\n",
      "11, Colorado          , npdfs =  23       , total_pdfpages = 129\n",
      "12, Arkansas          , npdfs =  70       , total_pdfpages = 496\n",
      "13, Mississipi        , npdfs =  9        , total_pdfpages = 26\n",
      "14, Texas             , npdfs =  32       , total_pdfpages = 294\n",
      "15, Virginia          , npdfs =  4        , total_pdfpages = 36\n",
      "16, Minnesota         , npdfs =  1        , total_pdfpages = 8\n",
      "17, North_Carolina    , npdfs =  13       , total_pdfpages = 216\n",
      "18, New_Jersey        , npdfs =  8        , total_pdfpages = 38\n",
      "19, Louisiana         , npdfs =  0        , total_pdfpages = 0\n",
      "20, Michigan          , npdfs =  2        , total_pdfpages = 378\n",
      "21, Oklahoma          , npdfs =  28       , total_pdfpages = 112\n",
      "22, Florida           , npdfs =  43       , total_pdfpages = 274\n",
      "23, Wyoming           , npdfs =  5        , total_pdfpages = 103\n",
      "24, Kansas            , npdfs =  228      , total_pdfpages = 2112\n",
      "25, California        , npdfs =  88       , total_pdfpages = 285\n",
      "26, Kentucky          , npdfs =  30       , total_pdfpages = 190\n",
      "27, New_York          , npdfs =  0        , total_pdfpages = 0\n",
      "28, Oregon            , npdfs =  18       , total_pdfpages = 420\n",
      "29, Georgia           , npdfs =  0        , total_pdfpages = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:PyPDF2._reader:Overwriting cache for 0 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30, Utah              , npdfs =  23       , total_pdfpages = 264\n",
      "31, Arizona           , npdfs =  38       , total_pdfpages = 582\n",
      "32, Illinois          , npdfs =  2        , total_pdfpages = 56\n",
      "33, Montana           , npdfs =  3        , total_pdfpages = 49\n",
      "34, Washington        , npdfs =  8        , total_pdfpages = 93\n",
      "35, Missouri          , npdfs =  1        , total_pdfpages = 330\n",
      "36, Nevada            , npdfs =  1        , total_pdfpages = 15\n",
      "37, South_Dakota      , npdfs =  56       , total_pdfpages = 440\n",
      "38, Ohio              , npdfs =  0        , total_pdfpages = 0\n",
      "39, Delaware          , npdfs =  0        , total_pdfpages = 0\n"
     ]
    }
   ],
   "source": [
    "pdfs_dirpaths = []\n",
    "records_pdf = dict()\n",
    "records_txt = dict()\n",
    "for i, dir in enumerate(glob(f\"{states_main_dir}/*\")):\n",
    "    state = dir.split(os.path.sep)[-1]\n",
    "    npdfs, total_pdfpages = ut.get_total_pdfpages_in_folder(dir)\n",
    "    if npdfs > 0:\n",
    "        pdfs_dirpaths.append(dir)\n",
    "        records_pdf[state] = glob(f\"{dir}/*.pdf\")\n",
    "    else:\n",
    "        records_txt[state] = glob(f\"{dir}/*.txt\")\n",
    "    print(f\"{i:2d}, {state:<18}, {npdfs = : < 10}, {total_pdfpages = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(input_text):\n",
    "    # Remove empty lines\n",
    "    non_empty_lines = [line.strip() for line in input_text.splitlines() if line.strip()]\n",
    "\n",
    "    # Define the list of keywords to stop processing lines after\n",
    "    stop_keywords = ['Reviewers', 'Acknowledgements', 'Reference', 'References', 'Sources',]\n",
    "    url_pattern = r'\\b(?:http|https|ftp|www)\\S*|\\S+\\.\\S+\\/\\S*'\n",
    "    www_pattern = r'^www\\b'\n",
    "\n",
    "    cleaned_lines = []\n",
    "    skip = False\n",
    "\n",
    "    for line in non_empty_lines:\n",
    "        if line.startswith(('Author', 'Authors', 'The authors')):\n",
    "            continue\n",
    "        if any(keyword in line for keyword in stop_keywords):\n",
    "            skip = True\n",
    "        if not skip:\n",
    "            if not re.search(url_pattern, line) or re.search(www_pattern, line):\n",
    "                line_fixed = line.replace(\"- \", \"\")\n",
    "                cleaned_lines.append(line_fixed)\n",
    "    \n",
    "    cleaned_text = '\\n'.join(cleaned_lines)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. AWS Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textractor import Textractor\n",
    "from textractor.visualizers.entitylist import EntityList\n",
    "from textractor.data.constants import TextractFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_region = 'us-east-1'\n",
    "s3_client = boto3.client('s3', region_name=aws_region)\n",
    "textract_client = boto3.client('textract', region_name=aws_region)\n",
    "\n",
    "extractor_obj = Textractor(region_name=aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an S3 bucket and upload pdfs to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_s3_bucket(s3, bucket_name, aws_region):\n",
    "    try:\n",
    "        # Create a bucket in the specified region\n",
    "        s3.create_bucket(Bucket=bucket_name,)\n",
    "        print(f\"Bucket '{bucket_name}' created successfully in region '{aws_region}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating bucket '{bucket_name}': {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'ae-corpora-bucket' created successfully in region 'us-east-1'.\n"
     ]
    }
   ],
   "source": [
    "new_bucket_name = \"ae-corpora-bucket\"\n",
    "create_s3_bucket(s3_client, new_bucket_name, aws_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textractor import Textractor\n",
    "from textractor.visualizers.entitylist import EntityList\n",
    "from textractor.data.constants import TextractFeatures\n",
    "from textractor.data.text_linearization_config import TextLinearizationConfig\n",
    "\n",
    "def pdf2text_aws_textract(pdf_file_path, extractor_obj, s3_bucket_name, out_dir=CLEAN_OUT_DIR):\n",
    "\n",
    "    if pdf_file_path is None or (pdf_file_path[-3:] not in [\"pdf\", \"PDF\"]):\n",
    "        print(f\"Not a pdf file: {pdf_file_path}\")\n",
    "        return\n",
    "   \n",
    "    try:\n",
    "        document = extractor_obj.start_document_analysis(\n",
    "            file_source=pdf_file_path,\n",
    "            s3_upload_path=f\"s3://{s3_bucket_name}/\",\n",
    "            features=[TextractFeatures.LAYOUT],\n",
    "        )\n",
    "\n",
    "        config_postprocess = TextLinearizationConfig(\n",
    "            hide_figure_layout=True,\n",
    "            hide_header_layout=True,\n",
    "            hide_footer_layout=True,\n",
    "            hide_page_num_layout=True,\n",
    "            linearize_table=False,\n",
    "        )\n",
    "\n",
    "        cleaned_text = document.get_text(config=config_postprocess)\n",
    "        state = pdf_file_path.split(os.path.sep)[-2]\n",
    "        fname = pdf_file_path.split(os.path.sep)[-1].replace(\"pdf\", \"txt\")\n",
    "        txt_outpath = os.path.join(out_dir, f\"{state}_{fname}\")\n",
    "        \n",
    "        with open(txt_outpath, 'w', encoding='utf-8') as text_file:\n",
    "            text_file.write(clean_text(cleaned_text))\n",
    "\n",
    "        print(f\"Successfully created: {txt_outpath}\")\n",
    "        return txt_outpath\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "\n",
    "def pdf2text_aws_textract_in_batch(state, extractor_obj, s3_bucket_name, records, out_dir=CLEAN_OUT_DIR):\n",
    "    pdfs_list_state = records.get(state, None)\n",
    "    for pdf_file_path in pdfs_list_state:\n",
    "        pdf2text_aws_textract(pdf_file_path, extractor_obj, s3_bucket_name)\n",
    "    \n",
    "\n",
    "def simple_txt2txt(txt_file_path, out_dir=CLEAN_OUT_DIR):\n",
    "    if txt_file_path[-3:] != \"txt\":\n",
    "        print(f\"Not a txt file: {txt_file_path}\")\n",
    "        return\n",
    "\n",
    "    fname = txt_file_path.split(os.path.sep)[-1]\n",
    "    state = txt_file_path.split(os.path.sep)[-2]\n",
    "    txt_outpath = os.path.join(out_dir, f\"{state}_{fname}\")\n",
    "\n",
    "    with open(txt_file_path, \"r\") as fr:\n",
    "        with open(txt_outpath, 'w', encoding='utf-8') as text_file:\n",
    "            text_file.write(clean_text(fr.read()))   \n",
    "\n",
    "    return txt_outpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_records_pdf = dict(sorted(records_pdf.items(), key=lambda item: len(item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will begin conversion of   1 documents for: New_Mexico        \n",
      "Will begin conversion of   1 documents for: Minnesota         \n",
      "Will begin conversion of   1 documents for: Missouri          \n",
      "Will begin conversion of   1 documents for: Nevada            \n",
      "Will begin conversion of   2 documents for: Michigan          \n",
      "Will begin conversion of   2 documents for: Illinois          \n",
      "Will begin conversion of   3 documents for: Maryland          \n",
      "Will begin conversion of   3 documents for: Montana           \n",
      "Will begin conversion of   4 documents for: Virginia          \n",
      "Will begin conversion of   5 documents for: North_Dakota      \n",
      "Will begin conversion of   5 documents for: Wyoming           \n",
      "Will begin conversion of   8 documents for: New_Jersey        \n",
      "Will begin conversion of   8 documents for: Washington        \n",
      "Will begin conversion of   9 documents for: Mississipi        \n",
      "Will begin conversion of  13 documents for: North_Carolina    \n",
      "Will begin conversion of  18 documents for: Oregon            \n",
      "Will begin conversion of  23 documents for: Colorado          \n",
      "Will begin conversion of  23 documents for: Utah              \n",
      "Will begin conversion of  28 documents for: Oklahoma          \n",
      "Will begin conversion of  30 documents for: Kentucky          \n",
      "Will begin conversion of  31 documents for: Tennessee         \n",
      "Will begin conversion of  32 documents for: Texas             \n",
      "Will begin conversion of  34 documents for: Wisconsin         \n",
      "Will begin conversion of  38 documents for: Arizona           \n",
      "Will begin conversion of  43 documents for: Florida           \n",
      "Will begin conversion of  56 documents for: South_Dakota      \n",
      "Will begin conversion of  70 documents for: Arkansas          \n",
      "Will begin conversion of  88 documents for: California        \n",
      "Will begin conversion of 228 documents for: Kansas            \n"
     ]
    }
   ],
   "source": [
    "for state, pdfs_list_state in sorted_records_pdf.items(): \n",
    "    print(f\"Will begin conversion of {len(pdfs_list_state):3} documents for: {state:<18}\")\n",
    "    start_time = time.monotonic()\n",
    "    pdf2text_aws_textract_in_batch(state, extractor_obj, new_bucket_name, records_pdf, out_dir=CLEAN_OUT_DIR)\n",
    "    duration = timedelta(seconds=time.monotonic() - start_time)\n",
    "    print(f\"Conversion took: {duration}\")\n",
    "    print(\"----------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "001_aequad_benchmark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
